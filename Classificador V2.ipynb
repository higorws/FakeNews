{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIBLIOTECAS E DEPENDENCIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, shutil, tweepy, re, nltk, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from autocorrect import spell\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, cross_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASE DE DADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpando a pasta para não ter o risco de somar ao inves de sobescrever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('C:/Users/higor.silva/TCC/base/true/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegando aleatoriamente os arquivos da base \"True\" para o nosso algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source= \"C:/Users/higor.silva/TCC/base/txtTribuna\"\n",
    "dest= \"C:/Users/higor.silva/TCC/base/true\"\n",
    "no_of_files=3000\n",
    "\n",
    "#print(\"%\"*25+\"{ Details Of Transfer }\"+\"%\"*25)\n",
    "#print(\"\\n\\nList of Files Moved to %s :-\"%(dest))\n",
    "\n",
    "for i in range(no_of_files):    \n",
    "    random_file=random.choice(os.listdir(source))\n",
    "    #print(\"%d} %s\"%(i+1,random_file))\n",
    "    source_file=\"%s\\%s\"%(source,random_file)\n",
    "    dest_file=dest    \n",
    "    shutil.copy(source_file,dest_file)\n",
    "\n",
    "#print(\"\\n\\n\"+\"$\"*33+\"[ Files Moved Successfully ]\"+\"$\"*33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta = 'base'\n",
    "labels = {'true': 1, 'fake': 0}\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na célula abaixo é feito um processo de unificação dos arquivos .txt para um unico arquivo .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ('true', 'fake'):\n",
    "    path = os.path.join(pasta,f)\n",
    "    for file in os.listdir (path) :\n",
    "        with open(os.path.join(path, file),'r', encoding='ISO-8859-1') as infile:\n",
    "            txt = infile.read()\n",
    "        df = df.append([[txt, labels[f]]],ignore_index=True)\n",
    "df.columns = ['noticia', 'rotulo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('noticias_data.csv', index=False, encoding='utf-8' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "df.groupby('rotulo').rotulo.count().plot.bar(ylim=0)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TWITTER\n",
    "\n",
    "Nesta parte lidaremos com o Twitter diretamente, onde faremos a coleta do tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler('V3MjFGhabsXp4fyEEEaicqMPR', 'onmrKkii8JjA5840pZwdH8Rmkad6QJFBxuhr4hLnJ7jB8h01p0')\n",
    "auth.set_access_token('978402798416093185-DF3ig67yuKhkQcVx7PoOBTi4w56Jh2j',\n",
    "        'nLRZgOiHZGotiZ7FjzR2RkL39bMZFiMm2G7X9NeJDruQz')\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_tweets(usuario, limite=10):\n",
    "    resultados = api.user_timeline(screen_name=usuario, count=limite, tweet_mode='extended')\n",
    "    tweets = [] \n",
    "    for r in resultados:\n",
    "        tweet = re.sub(r'http\\S+', '', r.full_text)\n",
    "        tweets.append(tweet.replace('\\n', ' ')) \n",
    "    return tweets \n",
    "tweets2 = obter_tweets(usuario='@GuilhermeBoulos', limite=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tweets2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unificando os tweets recolhidos com a nossa base de apredizado, pois todos passarão juntos pale parte de tratamento de texto e Bag Of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(tweets2)\n",
    "df2.columns = ['noticia']\n",
    "#print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = pd.concat([df, df2],ignore_index=True, sort = False)\n",
    "frames = frames.replace(np.nan, 0)\n",
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames.iloc[6254:6269,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRÉ-PROSSEMANTO DE TEXTO\n",
    "\n",
    "Nesta segunda parte é onde é feita o pré processamento do texto. Aqui é aplicado o Stop Words e Stemming/lematização. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "arq_stopwords = open(\"ListaStopWords.txt\", \"r\", encoding='utf-8')\n",
    "stopwords = arq_stopwords.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicastemmer(texto):    \n",
    "    frases_processed = []    \n",
    "    for palavra in texto:\n",
    "        comstemming = [str(stemmer.stem(p)) for p in palavra.split() if p not in stopwords]\n",
    "        frases_processed.append(comstemming)            \n",
    "    return frases_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticiario = frames['noticia'].str.lower()\n",
    "noticia = aplicastemmer(noticiario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(noticia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BAG OF WORDS\n",
    "\n",
    "Nesta parte é feita a bag of words, onde o algoritmo a baixo pega nosso documento(após o stemming) e converte para uma lista de vetores de incidencia, na qual pontua as palavras que mais se repete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "\n",
    "X = matrix.fit_transform(noticia).toarray()\n",
    "Y = frames['rotulo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TREINAMENTO\n",
    "\n",
    "Nesta parte é feito o treinamento do algoritmo com a nossa base. O algoritmo escolhido para efetuar o treinamento é o KNN em cima da metrica do cosseno. \n",
    "\n",
    "E definido a quantidade de K-vizinhos e logo a seguir é feita a separação em porcentagem da base teste e treino. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(metric = 'cosine', n_neighbors=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(x_train,y_train)\n",
    "pred = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred2 = knn.predict(X[6242:6256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter = frames.iloc[6254:6269,0]\n",
    "text_features = X[6254:6269]\n",
    "predictions = knn.predict(text_features)\n",
    "\n",
    "for text, predicted in zip(twitter, predictions):            \n",
    "    print('\"{}\"'.format(text))\n",
    "    print(\"  - Predicted as: '{}'\".format(predicted))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pont = metrics.classification_report(y_test,pred,target_names = [ 'false','true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_pont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CROSS - VALIDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL = cross_validate(knn, X, Y, cv=10, return_train_score=True, scoring=['f1','precision','recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VAL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
